{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RoadLoadSeq' from 'CustomDataset' (/home/dl08e22/roadkill_detection/CustomDataset.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1680285/1182590538.py\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mCustomDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRoadLoad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRoadLoadSeq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeeplabv3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeepLabHead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'RoadLoadSeq' from 'CustomDataset' (/home/dl08e22/roadkill_detection/CustomDataset.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from pathlib import Path\n",
    "from CustomDataset import CustomDataset, RoadLoad, RoadLoad_seq\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "import time\n",
    "import click\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import csv\n",
    "import ray\n",
    "from ray import tune\n",
    "import pandas as pd\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import shutil\n",
    "import gc\n",
    "torch.manual_seed(0)\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "gpu_usage()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(torch.cuda.current_device())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchvision import models\n",
    "def createDeepLabv3(outputchannels=1):\n",
    "    model = torchvision.models.segmentation.deeplabv3_resnet50(weights='DeepLabV3_ResNet50_Weights.DEFAULT',progress=True)\n",
    "    model.classifier = DeepLabHead(2048,outputchannels)\n",
    "    model.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloader\n",
    "normalize = transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                    std=[0.229,0.224,0.225])\n",
    "data_transforms = {\n",
    "    'Train':\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'Validation':\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "}\n",
    "dataset = CustomDataset(target_type = 'poly')\n",
    "\n",
    "batch_size = 2\n",
    "TRAIN_SIZE= math.floor(dataset.__len__()*0.70)\n",
    "VALIDATION_SIZE = math.floor((dataset.__len__() - TRAIN_SIZE)/2)\n",
    "TEST_SIZE = math.floor(dataset.__len__() - (TRAIN_SIZE+VALIDATION_SIZE))\n",
    "print([dataset.__len__(),TRAIN_SIZE, VALIDATION_SIZE, TEST_SIZE])\n",
    "image_datasets = {\n",
    "    'Train': \n",
    "        TRAIN_SIZE,\n",
    "    'Validation':\n",
    "        VALIDATION_SIZE,\n",
    "    'Test':\n",
    "        TEST_SIZE}\n",
    "\n",
    "trainset, validationset, testset = random_split(dataset,[TRAIN_SIZE, VALIDATION_SIZE, TEST_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, dataloaders, optimizer, metrics, bpath,\n",
    "                num_epochs,weights_name, device):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "    # Use gpu if available\n",
    "    # Initialize the log file for training and Validationing loss and metrics\n",
    "    fieldnames = ['epoch', 'Train_loss', 'Validation_loss'] + \\\n",
    "        [f'Train_{m}' for m in metrics.keys()] + \\\n",
    "        [f'Validation_{m}' for m in metrics.keys()]\n",
    "    with open(os.path.join(bpath, 'log.csv'), 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        #gpu_usage()\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "        # Each epoch has a training and Validation phase\n",
    "        # Initialize batch summary\n",
    "        batchsummary = {a: [0] for a in fieldnames}\n",
    "\n",
    "        for phase in ['Train', 'Validation']:\n",
    "            if phase == 'Train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            # Iterate over data.\n",
    "            for (inputs,label) in tqdm(iter(dataloaders[phase])):\n",
    "                inputs = inputs.to(device)\n",
    "                label = label.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'Train'):\n",
    "                    outputs = model(inputs)\n",
    "                    #loss = criterion(outputs['out'].squeeze(), label.squeeze())\n",
    "                    loss = torchvision.ops.sigmoid_focal_loss(outputs['out'].squeeze(), torch.squeeze(label), reduction = 'mean')\n",
    "                    y_pred = outputs['out'].detach().cpu().numpy()\n",
    "                    y_pred = np.squeeze(y_pred)\n",
    "                    y_true = label.detach().cpu().numpy()\n",
    "                    for name, metric in metrics.items():\n",
    "                        y_true = y_true.ravel()\n",
    "                        y_pred = y_pred.ravel()\n",
    "                        #print(y_true.shape, y_pred.shape)\n",
    "                        #print(np.max(y_true), np.min(y_true), np.max(y_pred),np.min(y_pred))\n",
    "                        try:\n",
    "                            if name == 'f1_score':\n",
    "                                # Use a classification threshold of 0.1\n",
    "                                batchsummary[f'{phase}_{name}'].append(\n",
    "                                    metric(y_true > 0, y_pred > 0.1))\n",
    "                            else:\n",
    "                                batchsummary[f'{phase}_{name}'].append(\n",
    "                                    metric(y_true.astype('uint8'), y_pred))\n",
    "                        except:\n",
    "                            batchsummary[f'{phase}_{name}'].append(0)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'Train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                inputs = None\n",
    "                label = None\n",
    "            batchsummary['epoch'] = epoch\n",
    "            epoch_loss = loss\n",
    "            batchsummary[f'{phase}_loss'] = epoch_loss.item()\n",
    "            print('{} Loss: {:.4f}'.format(phase, loss))\n",
    "        for field in fieldnames[3:]:\n",
    "            batchsummary[field] = np.mean(batchsummary[field])\n",
    "        print(batchsummary)\n",
    "        with open(os.path.join(bpath, 'log.csv'), 'a', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writerow(batchsummary)\n",
    "            # deep copy the model\n",
    "            if phase == 'Validation' and loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model, os.path.join(bpath, weights_name))\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Lowest Loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(exp_directory, epochs, batch_size, device, weights_name, lr = 1e-4, betas = (0.9,0.9999)):\n",
    "    # Create the deeplabv3 resnet101 model which is pretrained on a subset\n",
    "    # of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n",
    "    \n",
    "    train_dataloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    validation_dataloader = DataLoader(validationset, batch_size=batch_size, shuffle=True)\n",
    "    dataloaders = {\n",
    "        'Train':\n",
    "            train_dataloader,\n",
    "        'Validation':\n",
    "            validation_dataloader\n",
    "    }\n",
    "    \n",
    "\n",
    "    model = createDeepLabv3().to(device)\n",
    "    #print(model)\n",
    "    model.train()\n",
    "    # Create the experiment directory if not present\n",
    "    exp_directory = Path(exp_directory)\n",
    "    if not exp_directory.exists():\n",
    "        exp_directory.mkdir()\n",
    "    \n",
    "    # Specify the loss function\n",
    "    criterion = torch.nn.MSELoss(reduction='mean')\n",
    "    # Specify the optimizer with a lower learning rate\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr, betas = betas)\n",
    "\n",
    "    # Specify the evaluation metrics\n",
    "    metrics = {'f1_score': f1_score, 'auroc': roc_auc_score}\n",
    "\n",
    "    _ = train_model(model,\n",
    "                    criterion,\n",
    "                    dataloaders,\n",
    "                    optimizer,\n",
    "                    device = device,\n",
    "                    bpath=exp_directory,\n",
    "                    metrics=metrics,\n",
    "                    num_epochs=epochs,\n",
    "                    weights_name = weights_name)\n",
    "\n",
    "    # Save the trained model\n",
    "    #torch.save(model, exp_directory / 'weights_deeplab_2.pt')\n",
    "    \n",
    "\n",
    "exp_directory = 'models/pytorch/'\n",
    "weights_name = \"weights_deeplab_test2.pt\"\n",
    "#\n",
    "#main(exp_directory, epochs = 1, batch_size = 2, device=device, weights_name = weights_name, lr = 0.0001, betas = (0.9,0.9999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter search:\n",
    "exp_directory = 'models/pytorch/hpt/'\n",
    "lr_list = [1e-1, 1e-2, 1e-4]\n",
    "beta_list = [(0.9,0.9999),(0.7,0.9999),(0.9999,0.7)]\n",
    "epochs = 50\n",
    "def hpt(exp_directory, lr_list, epochs, bs_list):\n",
    "    for lr in lr_list:\n",
    "        for betas in beta_list:\n",
    "            name = \"lr\" + str(lr)+ \"_\" + \"beta\"+str(betas)\n",
    "            name_pt = name + \".pt\"\n",
    "            print(name)\n",
    "            main(exp_directory, epochs = epochs, batch_size = batch_size, device=device,weights_name = name_pt, lr = lr, betas = betas)\n",
    "            model = None\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            #rename\n",
    "            os.rename(Path(exp_directory+'log.csv').resolve(), Path(exp_directory+ name+ '.csv').resolve())\n",
    "    print(\"done\")\n",
    "#hpt(exp_directory, lr_list, epochs, beta_list)\n",
    "# lr0.1_beta(0.9999,0.7) er fyldt med nan, starter fra 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameter evaluation:\n",
    "def plot_loss_hpt(path_to_dir):\n",
    "    filenames = os.listdir(path_to_dir)\n",
    "    filenames = [ os.path.join(path_to_dir, filename) for filename in filenames if filename.endswith(\".csv\") ]\n",
    "    y_low = 1\n",
    "    for filename in filenames:\n",
    "        y = pd.read_csv(filename)[\"Validation_loss\"].values\n",
    "        #print(filename, y)\n",
    "        x = np.arange(0,len(y))+1\n",
    "        name = os.path.splitext(os.path.basename(filename))[0]\n",
    "        #print(name, y)\n",
    "        #with plt.style.context(\"fivethirtyeight\"):\n",
    "        line = plt.plot(x,y,label=name)\n",
    "        if np.min(y) < y_low:\n",
    "            y_low = np.min(y)\n",
    "            filename_low = filename\n",
    "            \n",
    "    print(filename_low, y_low)\n",
    "    plt.legend()\n",
    "    #plt.rcParams['figure.figsize'] = [40, 2]\n",
    "    plt.show()\n",
    "    plt.savefig(\"hpt_plot.png\")\n",
    "    plt.close()\n",
    "        \n",
    "plot_loss_hpt(exp_directory+\"50 epochs/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting stats for the full model\n",
    "filename = os.path.join(\"models\",\"pytorch\",\"weights_deeplab_lr0.0001_beta_0.7_0.9999_200epochs.csv\")\n",
    "\n",
    "#filename = os.path.join(\"models\",\"pytorch\",\"weights_deeplab_3_lr0.0001_beta_0.9_0.9999.csv\")\n",
    "y = pd.read_csv(filename)[\"Validation_auroc\"].values\n",
    "x = np.arange(len(y))\n",
    "plt.plot(x,y, label=\"Val_auroc\")\n",
    "y = pd.read_csv(filename)[\"Validation_f1_score\"].values\n",
    "x = np.arange(len(y))\n",
    "plt.plot(x,y, label=\"Val_f1\")\n",
    "plt.legend()\n",
    "plt.savefig('DL_auroc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def plot_images_eval(n, model, DataLoader):\n",
    "    torch.cuda.empty_cache()\n",
    "    for batch, (t_image, mask) in enumerate(DataLoader):\n",
    "        outputs = model(t_image)\n",
    "        y_pred = outputs['out']\n",
    "        y_pred = np.squeeze(y_pred > 0.1)\n",
    "        plt1 = plt.figure()\n",
    "        plt1 = plt.imshow(y_pred[0])\n",
    "        plt2 = plt.figure()\n",
    "        plt2 = plt.imshow(mask.data[0])\n",
    "        plt3 = plt.figure()\n",
    "        image = t_image[0].T\n",
    "        image = torch.movedim(image,(0,1,2),(1,0,2)).numpy()\n",
    "        image = (image).astype(np.uint8)\n",
    "        image = image[...,::-1]\n",
    "\n",
    "        i0 = image[:,:,0][:,:,np.newaxis]\n",
    "        i1 = image[:,:,1][:,:,np.newaxis]\n",
    "        i2 = image[:,:,2][:,:,np.newaxis]\n",
    "        new_image = np.concatenate((i2,i1),axis = 2)\n",
    "        new_image = np.concatenate((new_image,i0),axis = 2)\n",
    "        plt3 = plt.imshow(new_image)\n",
    "        if batch+1 == n:\n",
    "            break      \n",
    "model = torch.load('models/pytorch/weights_deeplab_4.pt')\n",
    "model.to(\"cpu\");\n",
    "test_dataloader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "#plot_images_eval(10, model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def plot_images_flat_eval(n, model, DataLoader,title = False):\n",
    "    torch.cuda.empty_cache()\n",
    "    #change save fodler here!!\n",
    "    path='plots/images/fcn/'\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError as error:\n",
    "        print(error)    \n",
    "    for batch, (t_image, mask) in enumerate(DataLoader):\n",
    "        outputs = model(t_image)\n",
    "        y_pred = outputs['out']\n",
    "        y_pred = np.squeeze(y_pred > 0.1)\n",
    "        print(batch)\n",
    "        for i in range(len(y_pred)):\n",
    "            fig,(ax1,ax2,ax3) = plt.subplots(1,3)\n",
    "            \n",
    "            ax1.imshow(y_pred[i])\n",
    "            if title: #implement name\n",
    "                fig.suptitle()\n",
    "                fig.tight_layout();\n",
    "                plt.subplots_adjust(top=1.2);\n",
    "            else:\n",
    "                fig.suptitle('pred vs target vs real image')\n",
    "                fig.tight_layout();\n",
    "                plt.subplots_adjust(top=1.4);\n",
    "            \n",
    "            ax2.imshow(mask.data[i])\n",
    "            image = t_image[i].T\n",
    "            image = torch.movedim(image,(0,1,2),(1,0,2)).numpy()\n",
    "            image = (image).astype(np.uint8)\n",
    "            image = image[...,::-1]\n",
    "\n",
    "            i0 = image[:,:,0][:,:,np.newaxis]\n",
    "            i1 = image[:,:,1][:,:,np.newaxis]\n",
    "            i2 = image[:,:,2][:,:,np.newaxis]\n",
    "            new_image = np.concatenate((i2,i1),axis = 2)\n",
    "            new_image = np.concatenate((new_image,i0),axis = 2)\n",
    "            ax3.imshow(new_image)\n",
    "            \n",
    "\n",
    "            save_path = os.path.join(path,str(batch*2+i))\n",
    "            fig.savefig(save_path)\n",
    "        if batch+1 == n:\n",
    "            break\n",
    "            \n",
    "model = torch.load('models/pytorch/fcn_resnet50_200epoch.pt')\n",
    "#model = torch.load('models/pytorch/weights_deeplab_lr0.0001_beta_0.7_0.9999_200epochs.pt')\n",
    "model.to(\"cpu\");\n",
    "test_dataloader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "#lot_images_flat_eval(10, model, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('models/pytorch/weights_deeplab_lr0.0001_beta_0.7_0.9999_200epochs.pt')\n",
    "model.to(\"cpu\");\n",
    "print(\"HEJ\")\n",
    "#plot_images_flat_eval(10, model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def plot_road_images_eval(n, model, DataLoader, plot_tgt = True, title = True):\n",
    "    torch.cuda.empty_cache()\n",
    "    #change save folder here!!\n",
    "    path='plots/road/fcn_road/'\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError as error:\n",
    "        print(error)   \n",
    "    for batch, (t_image, mask, name) in enumerate(DataLoader):\n",
    "        outputs = model(t_image)\n",
    "        y_pred = outputs['out']\n",
    "        y_pred = np.squeeze(y_pred > 0.1)\n",
    "        \n",
    "        for i in range(len(y_pred)):\n",
    "            fig, (ax1,ax2) = plt.subplots(1,2)\n",
    "            #print(\"a\")\n",
    "            #ax1.figure()\n",
    "            #print(t_image[0])\n",
    "            ax1.imshow(y_pred[i])\n",
    "            if title:\n",
    "                fig.suptitle(name[i]);\n",
    "                fig.tight_layout();\n",
    "                plt.subplots_adjust(top=1.2);\n",
    "            if plot_tgt:\n",
    "                #print(\"b\")\n",
    "                plt2 = plt.figure()\n",
    "                plt2 = plt.imshow(mask.data[i])\n",
    "                #if title:\n",
    "                    #plt2 = ax1.set_title(name[i])\n",
    "\n",
    "            #print(\"c\")\n",
    "            #ax2.figure()\n",
    "            image = t_image[i].T\n",
    "            image = torch.movedim(image,(0,1,2),(1,0,2)).numpy()\n",
    "            image = (image).astype(np.uint8)\n",
    "            image = image[...,::-1] # flip rgb\n",
    "            i0 = image[:,:,0][:,:,np.newaxis]\n",
    "            i1 = image[:,:,1][:,:,np.newaxis]\n",
    "            i2 = image[:,:,2][:,:,np.newaxis]\n",
    "            new_image = np.concatenate((i2,i1),axis = 2)\n",
    "            new_image = np.concatenate((new_image,i0),axis = 2)\n",
    "            #print(i0.shape)\n",
    "            ax2.imshow(new_image)\n",
    "            #print(image[:,:,:])\n",
    "            \n",
    "            save_path = os.path.join(path,str(batch*2+i))\n",
    "            fig.savefig(save_path)\n",
    "        if batch+1 == n:\n",
    "            break      \n",
    "model = torch.load('models/pytorch/weights_deeplab_3_lr0.0001_beta_0.9_0.9999.pt')\n",
    "model.to(\"cpu\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_images_eval(2, model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameter evaluation:\n",
    "def plot_loss_csv(csv):\n",
    "    csv_path = os.path.join(\"models\",\"pytorch\", csv)\n",
    "    y1 = pd.read_csv(csv_path)[\"Validation_loss\"].values\n",
    "    y2 = pd.read_csv(csv_path)[\"Train_loss\"].values\n",
    "    #print(filename, y)\n",
    "    x = np.arange(0,len(y1))+1\n",
    "    #print(name, y)\n",
    "    #with plt.style.context(\"fivethirtyeight\"):\n",
    "    plt.plot(x,y1,label='val')\n",
    "    plt.plot(x,y2,label='train')\n",
    "    plt.legend()\n",
    "    #plt.rcParams['figure.figsize'] = [40, 2]\n",
    "    \n",
    "    plt.savefig('plots/loss/deeplab_loss.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "csv = \"weights_deeplab_lr0.0001_beta_0.7_0.9999_200epochs.csv\"\n",
    "\n",
    "plot_loss_csv(csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on road-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RoadLoad_seq()\n",
    "model = torch.load('models/pytorch/weights_deeplab_lr0.0001_beta_0.7_0.9999_200epochs.pt')\n",
    "#model = torch.load('models/pytorch/fcn_resnet50_200epoch.pt')\n",
    "\n",
    "model.to(\"cpu\");\n",
    "road_loader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "plot_road_images_eval(10, model, road_loader, plot_tgt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea41beba1f519d1d4dcb6adfad9d34bac36624c7e369a0994f8dda19ed6843f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
