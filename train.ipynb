{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from CustomDataset import CustomDataset\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda-2022.05/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda-2022.05/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=FCN_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 174, 'validation': 58}\n",
      "Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1))\n",
      "FCNHead(\n",
      "  (0): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Dropout(p=0.1, inplace=False)\n",
      "  (4): Sequential(\n",
      "    (0): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Initializations\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model = torchvision.models.segmentation.fcn_resnet50(pretrained=True, progress = True).to(device)\n",
    "\n",
    "#has to normalize data the same way the pretrained images were\n",
    "normalize = transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                        std=[0.229,0.224,0.225])\n",
    "data_transforms = {\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'validation':\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "}\n",
    "dataset = CustomDataset(target_type = 'poly')\n",
    "\n",
    "TRAIN_SIZE= math.floor(dataset.__len__()*0.75)\n",
    "TEST_SIZE = dataset.__len__() - TRAIN_SIZE\n",
    "trainset, testset = random_split(dataset,[TRAIN_SIZE,TEST_SIZE])\n",
    "\n",
    "\n",
    "traindata_loader = DataLoader(trainset, batch_size=5, shuffle=True)\n",
    "testdata_loader = DataLoader(testset, batch_size=5, shuffle=True)\n",
    "LOADER_TRAIN_SIZE = traindata_loader.__len__()\n",
    "LOADER_TEST_SIZE = testdata_loader.__len__()\n",
    "\n",
    "image_datasets = {\n",
    "    'train': \n",
    "        LOADER_TRAIN_SIZE,\n",
    "    'validation':\n",
    "        LOADER_TEST_SIZE\n",
    "}\n",
    "print(image_datasets)\n",
    "dataloaders = {\n",
    "    'train':\n",
    "        traindata_loader,\n",
    "    'validation':\n",
    "        testdata_loader\n",
    "}\n",
    "# print(model)\n",
    "\n",
    "for name,param in model.named_parameters():\n",
    "    if 'classifier' not in name:\n",
    "        param.requires_grad = False  \n",
    "print(model.classifier[4])\n",
    "\n",
    "model.classifier[4] = nn.Sequential(\n",
    "    nn.Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1)).to(device),\n",
    "    nn.Sigmoid().to(device)\n",
    ")\n",
    "\n",
    "#model.fc = nn.Sequential(\n",
    "#               nn.Linear(2048, 128),\n",
    "#               nn.ReLU(inplace=True),\n",
    "#               nn.Linear(128, 2)).to(device)\n",
    "params_to_update = []\n",
    "for param in model.parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        \n",
    "class DiceLoss(torch.nn.Module):\n",
    "    def init(self):\n",
    "        super(diceLoss, self).init()\n",
    "    def forward(self,pred, target):\n",
    "        smooth = 1.\n",
    "        iflat = pred.contiguous().view(-1)\n",
    "        tflat = target.contiguous().view(-1)\n",
    "        intersection = (iflat * tflat).sum()\n",
    "        A_sum = torch.sum(iflat * iflat)\n",
    "        B_sum = torch.sum(tflat * tflat)\n",
    "        return 1 - ((2. * intersection + smooth) / (A_sum + B_sum + smooth) )\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "        \n",
    "        return Dice_BCE\n",
    "    \n",
    "    \n",
    "class FocalLoss(nn.CrossEntropyLoss):\n",
    "    ''' Focal loss for classification tasks on imbalanced datasets '''\n",
    "\n",
    "    def __init__(self, gamma, alpha=None, ignore_index=-100, reduction='none'):\n",
    "        super().__init__(weight=alpha, ignore_index=ignore_index, reduction='none')\n",
    "        self.reduction = reduction\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, input_, target):\n",
    "        cross_entropy = super().forward(input_, target)\n",
    "        # Temporarily mask out ignore index to '0' for valid gather-indices input.\n",
    "        # This won't contribute final loss as the cross_entropy contribution\n",
    "        # for these would be zero.\n",
    "        target = target * (target != self.ignore_index).long()\n",
    "        input_prob = torch.gather(F.softmax(input_, 1), 1, target.unsqueeze(1))\n",
    "        loss = torch.pow(1 - input_prob, self.gamma) * cross_entropy\n",
    "        return torch.mean(loss) if self.reduction == 'mean'else torch.sum(loss) if self.reduction == 'sum' else loss\n",
    "            \n",
    "            \n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "# loss_fn = FocalLoss(gamma=0.7)\n",
    "# loss_fn = torchvision.ops.sigmoid_focal_loss()\n",
    "# loss_fn = DiceBCELoss()\n",
    "# loss_fn = nn.NLLLoss()\n",
    "optim = optim.Adam(params_to_update, lr=0.001, eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "# optim = optim.Adam((model.classifier[4].parameters()))\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(prediction, truth):\n",
    "    \"\"\" Returns the confusion matrix for the values in the `prediction` and `truth`\n",
    "    tensors, i.e. the amount of positions where the values of `prediction`\n",
    "    and `truth` are\n",
    "    - 1 and 1 (True Positive)\n",
    "    - 1 and 0 (False Positive)\n",
    "    - 0 and 0 (True Negative)\n",
    "    - 0 and 1 (False Negative)\n",
    "    \"\"\"\n",
    "    threshold = torch.tensor([0.5]).to(device)\n",
    "    prediction = (prediction>threshold).float()*1\n",
    "    \n",
    "    confusion_vector = prediction / truth\n",
    "    # Element-wise division of the 2 tensors returns a new tensor which holds a\n",
    "    # unique value for each case:\n",
    "    #   1     where prediction and truth are 1 (True Positive)\n",
    "    #   inf   where prediction is 1 and truth is 0 (False Positive)\n",
    "    #   nan   where prediction and truth are 0 (True Negative)\n",
    "    #   0     where prediction is 0 and truth is 1 (False Negative)\n",
    "    # .item()\n",
    "    true_positives = torch.sum(confusion_vector == 1)\n",
    "    false_positives = torch.sum(confusion_vector == float('inf'))\n",
    "    true_negatives = torch.sum(torch.isnan(confusion_vector))\n",
    "    false_negatives = torch.sum(confusion_vector == 0)\n",
    "\n",
    "    return true_positives, false_positives, true_negatives, false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 480, 640])\n",
      "torch.Size([5, 480, 640])\n"
     ]
    }
   ],
   "source": [
    "for batch, (inputs,labels) in enumerate(dataloaders['train']):\n",
    "# for inputs,labels in enumerate(dataloaders[phase]):\n",
    "    # inputs = inputs.to(device)\n",
    "    # labels = labels.to(device)\n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "def train_model(model, optimizer, save_dir, num_epochs=3, evaluate = True):\n",
    "    # epoch_loss_list = []\n",
    "    # epoch_f1_list = []\n",
    "    # epoch_roc_list = []\n",
    "    phases = ['train']\n",
    "    if evaluate == True:\n",
    "        phases = ['train','validation']\n",
    "    train_performance = []\n",
    "    test_performance = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in phases:\n",
    "            print(\"Currently in the: \", phase,\" phase\")\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            total = 0\n",
    "            # running_corrects = 0\n",
    "            running_tp = 0\n",
    "            running_fp = 0\n",
    "            running_tn = 0\n",
    "            running_fn = 0\n",
    "            conf_tp = 0\n",
    "            conf_fp = 0\n",
    "            conf_tn = 0\n",
    "            conf_fn = 0\n",
    "            conf_precision = 0\n",
    "            conf_recall = 0\n",
    "            metrics_f1 = 0\n",
    "            metrics_roc = 0\n",
    "            # acc = 0\n",
    "            for batch, (inputs,labels) in enumerate(dataloaders[phase]):\n",
    "            # for inputs,labels in enumerate(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # print(inputs.dtype,labels.dtype)\n",
    "                # DON'T USE ARGMAX HERE\n",
    "                #labels = torch.argmax(labels, dim=0)\n",
    "                outputs = model(inputs)[\"out\"]\n",
    "                # print(outputs.shape, outputs.max(), outputs.min())\n",
    "                \n",
    "                preds = torch.squeeze(outputs).float().requires_grad_()\n",
    "                \n",
    "                loss = torchvision.ops.sigmoid_focal_loss(preds, torch.squeeze(labels), reduction = 'mean')\n",
    "                \n",
    "                # loss = criterion(outputs, labels.long())\n",
    "                # print(loss)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                # _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                total += labels.nelement()\n",
    "                # running_corrects += preds.eq(labels.data).sum().item()\n",
    "                # acc += running_corrects/total\n",
    "                \n",
    "                \n",
    "                #print(running_loss)\n",
    "                if (epoch == (num_epochs-1)) and (phase != 'train'):\n",
    "                    plt1 = plt.figure()\n",
    "                    plt1 = plt.imshow(preds.cpu().detach().numpy()[0])\n",
    "\n",
    "                    plt2 = plt.figure()\n",
    "                    plt2 = plt.imshow(labels.data.cpu()[0])\n",
    "                    \n",
    "                tp, fp, tn, fn = confusion(preds, labels.data)\n",
    "                # print(tp, fp, tn, fn)\n",
    "                \n",
    "                running_tp += tp\n",
    "                running_fp += fp\n",
    "                running_tn += tn\n",
    "                running_fn += fn\n",
    "                conf_tp += running_tp/total\n",
    "                conf_fp += running_fp/total\n",
    "                conf_tn += running_tn/total\n",
    "                conf_fn += running_fn/total\n",
    "                conf_precision += running_tp/(running_tp+running_fp)\n",
    "                conf_recall += running_tp/(running_tp+running_fn)\n",
    "                \n",
    "                # print(total)\n",
    "                # print(conf_tp, conf_fp, conf_tn, conf_fn, acc)\n",
    "                \n",
    "                metrics_f1 += f1_score(labels.cpu().ravel()>0, preds.cpu().ravel()>0.5)\n",
    "                metrics_roc += roc_auc_score(labels.cpu().int().ravel().detach().numpy(),preds.cpu().ravel().detach().numpy())\n",
    "                # print(metrics_f1, metrics_roc)\n",
    "            # Calculating loss and acc\n",
    "            \n",
    "            epoch_loss = running_loss / image_datasets[phase]\n",
    "            # epoch_loss_list.append(epoch_loss)\n",
    "            epoch_precision = conf_precision / image_datasets[phase]\n",
    "            epoch_recall = conf_recall / image_datasets[phase]\n",
    "            epoch_f1 = epoch_precision*epoch_recall/(epoch_precision+epoch_recall)\n",
    "            epoch_metrics_f1 = metrics_f1 / image_datasets[phase]\n",
    "            epoch_metrics_roc = metrics_roc / image_datasets[phase]\n",
    "            # epoch_f1_list.append(epoch_metrics_f1)\n",
    "            # epoch_roc_list.append(epoch_metrics_roc)\n",
    "            # epoch_confmat = (conf_tp, conf_fp, conf_tn, conf_fn)/image_datasets[phase]\n",
    "            # epoch_confmat_list.append(epoch_confmat)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                performance = [epoch, epoch_loss, epoch_metrics_f1, epoch_metrics_roc]\n",
    "            else:\n",
    "                performance.extend((epoch_loss, epoch_metrics_f1, epoch_metrics_roc))\n",
    "                print(performance)\n",
    "                with open(os.path.join(save_dir, 'fcn_log.csv'), 'a+', newline='') as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(performance)\n",
    "\n",
    "            \n",
    "            print('epoch: {} {} loss: {:.4f}, '.format(epoch,phase,\n",
    "                                                        epoch_loss))\n",
    "            print('f1 score: {:.4f}, precision: {:.4f}, recall: {:.4f}'.format(epoch_f1,\n",
    "                                                                              epoch_precision,\n",
    "                                                                              epoch_recall))\n",
    "            print('TP: {:.4f}, FP: {:.4f}, TN: {:.4f}, FN: {:.4f}'. format(\n",
    "                                                        conf_tp/image_datasets[phase],\n",
    "                                                        conf_fp/image_datasets[phase],\n",
    "                                                        conf_tn/image_datasets[phase],\n",
    "                                                        conf_fn/image_datasets[phase])\n",
    "                                                        )\n",
    "            best_loss = 1\n",
    "            if phase == 'validation' and loss < best_loss:\n",
    "                best_loss = loss\n",
    "                # best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model, os.path.join(save_dir, 'fcn_resnet50.pt'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently in the:  train  phase\n",
      "epoch: 0 train loss: 0.6352, \n",
      "f1 score: 0.2414, precision: 0.9867, recall: 0.3197\n",
      "TP: 0.0079, FP: 0.0001, TN: 0.9747, FN: 0.0169\n",
      "Currently in the:  validation  phase\n",
      "[0, 0.6352248346411634, 0.48297190997084355, 0.9724734836922172, 0.6345543154868586, 0.6540555529892148, 0.9900924456704636]\n",
      "epoch: 0 validation loss: 0.6346, \n",
      "f1 score: 0.3487, precision: 0.9708, recall: 0.5440\n",
      "TP: 0.0132, FP: 0.0004, TN: 0.9744, FN: 0.0112\n",
      "Currently in the:  train  phase\n",
      "epoch: 1 train loss: 0.6351, \n",
      "f1 score: 0.2813, precision: 0.9848, recall: 0.3938\n",
      "TP: 0.0097, FP: 0.0001, TN: 0.9747, FN: 0.0148\n",
      "Currently in the:  validation  phase\n",
      "[1, 0.6351487394208195, 0.49337540029428895, 0.9716774426735219, 0.6346842552824267, 0.5105842791073111, 0.9738574865016695]\n",
      "epoch: 1 validation loss: 0.6347, \n",
      "f1 score: 0.2683, precision: 0.9938, recall: 0.3675\n",
      "TP: 0.0108, FP: 0.0001, TN: 0.9703, FN: 0.0184\n",
      "Currently in the:  train  phase\n",
      "epoch: 2 train loss: 0.6350, \n",
      "f1 score: 0.2778, precision: 0.9837, recall: 0.3871\n",
      "TP: 0.0090, FP: 0.0001, TN: 0.9761, FN: 0.0144\n",
      "Currently in the:  validation  phase\n",
      "[2, 0.6350459127590574, 0.5252497085213942, 0.9710298421057443, 0.6343069212703869, 0.5281892687069626, 0.981521883574426]\n",
      "epoch: 2 validation loss: 0.6343, \n",
      "f1 score: 0.2751, precision: 0.9517, recall: 0.3870\n",
      "TP: 0.0098, FP: 0.0005, TN: 0.9737, FN: 0.0156\n",
      "Currently in the:  train  phase\n",
      "epoch: 3 train loss: 0.6349, \n",
      "f1 score: 0.2836, precision: 0.9831, recall: 0.3986\n",
      "TP: 0.0097, FP: 0.0002, TN: 0.9751, FN: 0.0146\n",
      "Currently in the:  validation  phase\n",
      "[3, 0.6348849959328942, 0.5512366508949973, 0.9744282791968752, 0.634658058655673, 0.632936014746245, 0.9876969126815592]\n",
      "epoch: 3 validation loss: 0.6347, \n",
      "f1 score: 0.3345, precision: 0.9579, recall: 0.5139\n",
      "TP: 0.0133, FP: 0.0006, TN: 0.9726, FN: 0.0126\n",
      "Currently in the:  train  phase\n",
      "epoch: 4 train loss: 0.6348, \n",
      "f1 score: 0.3022, precision: 0.9854, recall: 0.4358\n",
      "TP: 0.0104, FP: 0.0002, TN: 0.9755, FN: 0.0134\n",
      "Currently in the:  validation  phase\n",
      "[4, 0.6347873781689968, 0.5659574231030864, 0.9716026540062472, 0.6344311938717447, 0.497678137385863, 0.962711974789056]\n",
      "epoch: 4 validation loss: 0.6344, \n",
      "f1 score: 0.2573, precision: 0.9927, recall: 0.3472\n",
      "TP: 0.0087, FP: 0.0001, TN: 0.9736, FN: 0.0166\n",
      "Currently in the:  train  phase\n",
      "epoch: 5 train loss: 0.6349, \n",
      "f1 score: 0.2569, precision: 0.9876, recall: 0.3472\n",
      "TP: 0.0086, FP: 0.0001, TN: 0.9747, FN: 0.0161\n",
      "Currently in the:  validation  phase\n",
      "[5, 0.6348583940202477, 0.5360065640250459, 0.9716214279793173, 0.6340806524044481, 0.5940642429618205, 0.9833443863522121]\n",
      "epoch: 5 validation loss: 0.6341, \n",
      "f1 score: 0.3171, precision: 0.9730, recall: 0.4704\n",
      "TP: 0.0119, FP: 0.0003, TN: 0.9739, FN: 0.0134\n",
      "Currently in the:  train  phase\n",
      "epoch: 6 train loss: 0.6347, \n",
      "f1 score: 0.2947, precision: 0.9909, recall: 0.4195\n",
      "TP: 0.0104, FP: 0.0001, TN: 0.9746, FN: 0.0144\n",
      "Currently in the:  validation  phase\n",
      "[6, 0.6347136399869261, 0.5689382742409081, 0.9724460933054896, 0.6342613589147041, 0.5855847452768911, 0.9806684748833132]\n",
      "epoch: 6 validation loss: 0.6343, \n",
      "f1 score: 0.3232, precision: 0.9708, recall: 0.4846\n",
      "TP: 0.0147, FP: 0.0005, TN: 0.9683, FN: 0.0156\n",
      "Currently in the:  train  phase\n",
      "epoch: 7 train loss: 0.6345, \n",
      "f1 score: 0.3024, precision: 0.9930, recall: 0.4348\n",
      "TP: 0.0110, FP: 0.0001, TN: 0.9742, FN: 0.0143\n",
      "Currently in the:  validation  phase\n",
      "[7, 0.6345446520838244, 0.5985355773727556, 0.9737192760063418, 0.6340889780428903, 0.5852990432873671, 0.9812229076310814]\n",
      "epoch: 7 validation loss: 0.6341, \n",
      "f1 score: 0.3089, precision: 0.9843, recall: 0.4501\n",
      "TP: 0.0109, FP: 0.0002, TN: 0.9747, FN: 0.0134\n",
      "Currently in the:  train  phase\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(os.getcwd(),'models/pytorch')\n",
    "model_trained = train_model(model, optim, save_dir, num_epochs=15)\n",
    "#!mkdir models\n",
    "#!mkdir models/pytorch\n",
    "\n",
    "# torch.save(model_trained.state_dict(), 'models/pytorch/weights_dec04_fcn_resnet50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.segmentation.fcn_resnet50(pretrained=True)\n",
    "model.classifier[4] = nn.Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
    "model.load_state_dict(torch.load('models/pytorch/weights5.h5'))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_trained, epoch_loss_list, epoch_f1_list = train_model(model, loss_fn, optim, num_epochs=1, evaluate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = [0.1,0.2,0.3]\n",
    "with open(os.path.join(save_dir, 'fcn_log.csv'), 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(performance)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea41beba1f519d1d4dcb6adfad9d34bac36624c7e369a0994f8dda19ed6843f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
